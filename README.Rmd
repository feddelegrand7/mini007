---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# mini007 <a><img src='man/figures/mini007cute.png' align="right" height="200" /></a>

<!-- badges: start -->

[![CRAN
status](https://www.r-pkg.org/badges/version/mini007)](https://CRAN.R-project.org/package=mini007)
[![R
badge](https://img.shields.io/badge/Build%20with-‚ô•%20and%20R-blue)](https://github.com/feddelegrand7/mini007)
[![metacran
downloads](https://cranlogs.r-pkg.org/badges/mini007)](https://cran.r-project.org/package=mini007)
[![metacran
downloads](https://cranlogs.r-pkg.org/badges/grand-total/mini007)](https://cran.r-project.org/package=mini007)

<!-- badges: end -->

`mini007` provides a lightweight and extensible framework for multi-agents orchestration processes capable of decomposing complex tasks and assigning them to specialized agents. 

Each `agent` is an extension of an `ellmer` object. `mini007` relies heavily on the excellent `ellmer` package but aims to make it easy to create a process where multiple specialized agents help each other sequentially in order to execute a task.

`mini007` provides two types of agents: 

- A normal `Agent` containing a name and an instruction, 
- and a `LeadAgent` which will take a complex prompt, split it, assign to the adequate agents and retrieve the response. 

#### Highlights

üß† Memory and identity for each agent via `uuid` and message history.

‚öôÔ∏è Built-in task decomposition and delegation via `LLM`.

üîÑ Agent-to-agent orchestration with result chaining.

üåê Compatible with any chat model supported by `ellmer`.

üßë Possibility to set a Human In The Loop (`HITL`) at various execution steps

You can install `mini007` from `CRAN` with: 

```{r, eval=FALSE}
install.packages("mini007")
```

```{r}
library(mini007)
```


# Agent 
### Creating an Agent

An Agent is built upon an LLM object created by the `ellmer` package, in the following examples, we'll work with the `OpenAI` models, however you can use any model/combination of models you want: 

```{r}
# no need to provide the system prompt, it will be set when creating the
# agent (see the 'instruction' parameter)

retrieve_open_ai_credential <- function() {
  Sys.getenv("OPENAI_API_KEY")
}

openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

```

After initializing the `ellmer` LLM object, creating the Agent is straightforward: 

```{r}
polar_bear_researcher <- Agent$new(
  name = "POLAR BEAR RESEARCHER",
  instruction = "You are an expert in polar bears, you task is to collect information about polar bears. Answer in 1 sentence max.",
  llm_object = openai_4_1_mini
)
```

Each created Agent has an `agent_id` (among other meta information): 

```{r}
polar_bear_researcher$agent_id
```

At any time, you can tweak the `llm_object`: 
```{r}
polar_bear_researcher$llm_object
```

An agent can provide the answer to a prompt using the `invoke` method: 

```{r}
polar_bear_researcher$invoke("Are polar bears dangerous for humans?")
```


You can also retrieve a list that displays the history of the agent:

```{r}
polar_bear_researcher$messages
```
Or the `ellmer` way: 

```{r}
polar_bear_researcher$llm_object
```

## Agents and Messages

#### Managing Agent Conversation History

The `clear_and_summarise_messages` method allows you to compress an agent's conversation history into a concise summary and clear the message history while preserving context. This is useful for maintaining memory efficiency while keeping important conversation context.

```{r}
# After several interactions, summarise and clear the conversation history
polar_bear_researcher$clear_and_summarise_messages()
polar_bear_researcher$messages
```

This method summarises all previous conversations into a paragraph and appends it to the system prompt, then clears the conversation history. The agent retains the context but with reduced memory usage.

#### Keep only the most recent messages with `keep_last_n_messages()`

When a conversation grows long, you can keep just the last N messages while preserving the system prompt. This helps control token usage without fully resetting context.

```{r}
openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

agent <- Agent$new(
  name = "history_manager",
  instruction = "You are a concise assistant.",
  llm_object = openai_4_1_mini
)

agent$invoke("What is the capital of Italy?")
agent$invoke("What is the capital of Germany?")
agent$invoke("What is the capital of Algeria?")
agent$messages
```

```{r}
# Keep only the last 2 messages (system prompt is preserved)
agent$keep_last_n_messages(n = 2)
agent$messages
```
#### Manually Adding Messages to an Agent‚Äôs History

You can inject any message (system, user, or assistant) directly into an Agent‚Äôs history with `add_message(role, content)`. This is helpful to reconstruct, supplement, or simulate conversation steps.

- **add_message(role, content)**:
  - `role`: "user", "assistant", or "system"
  - `content`: The text message to add

```{r, eval=TRUE}
openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)
agent <- Agent$new(
  name = "Pizza expert",
  instruction = "You are a Pizza expert",
  llm_object = openai_4_1_mini
)

# Add a user message, an assistant reply, and a system instruction:
agent$add_message("user", "Where can I find the best pizza in the world?")
agent$add_message("assistant", "You can find the best pizza in the world in Algiers, Algeria. It's tasty and crunchy.")

# View conversation history
agent$messages
```

This makes it easy to reconstruct or extend sessions, provide custom context, or insert notes for debugging/testing purposes.

```{r, eval=TRUE}
agent$invoke("summarise the previous conversation")
```

#### Sync between `messages` and `turns`

You can modify the `messages` object as you please, this will be automatically translated to the suitable `turns` required by `ellmer`: 

```{r}
agent$messages[[5]]$content <- "Obivously you asked me about the best pizza in the world which is of course in Algiery!"

agent$messages
```
The underlying ellmer object: 

```{r}
agent$llm_object
```

#### Resetting conversation history

If you want to clear the conversation while preserving the current system prompt, use `reset_conversation_history()`.

```{r}
openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

agent <- Agent$new(
  name = "session_reset",
  instruction = "You are an assistant.",
  llm_object = openai_4_1_mini
)

agent$invoke("Tell me a short fun fact about dates (the fruit).")
agent$invoke("And one more.")

# Clear all messages except the system prompt
agent$reset_conversation_history()
agent$messages
```

#### Exporting and Loading Agent Conversation History

You can save an agent‚Äôs conversation history to a file and reload it later. This allows you to archive, transfer, or resume agent sessions across R sessions or machines.

- **export_messages_history(file_path)**: Saves the current conversation to a JSON file. 
- **load_messages_history(file_path)**: Loads a saved conversation history from a JSON file, replacing the agent‚Äôs current history.

In both methods, if you omit the `file_path` parameter, a default file named `"<getwd()>/<agent_name>_messages.json"` is used.

```{r, eval=FALSE}
openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)
agent <- Agent$new(
  name = "session_agent",
  instruction = "You are a persistent researcher.",
  llm_object = openai_4_1_mini
)

# Interact with the agent
agent$invoke("Tell me something interesting about volcanoes.")

# Save the conversation
agent$export_messages_history("volcano_session.json")

# ...Later, or in a new session...
# Restore the conversation
agent$load_messages_history("volcano_session.json")
# agent$messages  # Displays current history
```

#### Updating the system instruction during a session

Use `update_instruction(new_instruction)` to change the Agent‚Äôs system prompt mid-session. The first system message and the underlying `ellmer` system prompt are both updated.

```{r}
openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

agent <- Agent$new(
  name = "reconfigurable",
  instruction = "You are a helpful assistant.",
  llm_object = openai_4_1_mini
)

agent$update_instruction("You are a strictly concise assistant. Answer in one sentence.")

agent$messages
```

## Budget and cost control

You can limit how much an `Agent` is allowed to spend and decide what should happen as the budget is approached or exceeded. Use `set_budget()` to define the maximum spend (in USD), and `set_budget_policy()` to control warnings and over-budget behavior.

- **set_budget(amount_in_usd)**: sets the absolute budget for the agent.
- **set_budget_policy(on_exceed, warn_at)**:
  - **on_exceed**: one of `"abort"`, `"warn"`, or `"ask"`.
    - **abort**: stop with an error when the budget is exceeded.
    - **warn**: emit a warning and continue.
    - **ask**: interactively ask what to do when the budget is exceeded.
  - **warn_at**: a fraction in (0, 1); triggers a one-time warning when spending reaches that fraction of the budget (default `0.8`).

```{r}
# An API KEY is required to invoke the Agent
openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

agent <- Agent$new(
  name = "cost_conscious_assistant",
  instruction = "Answer succinctly.",
  llm_object = openai_4_1_mini
)

# Set a 5 USD budget
agent$set_budget(5)

# Warn at 90% of the budget and ask what to do if exceeded
agent$set_budget_policy(on_exceed = "ask", warn_at = 0.9)

# Normal usage
agent$invoke("Give me a one-sentence fun fact about Algeria.")
```

The current policy is echoed when setting the budget. You can update the policy at any time before or during an interaction lifecycle to adapt to your workflow's tolerance for cost overruns.

#### Inspecting usage and estimated cost

Call `get_usage_stats()` to retrieve the estimated cost, and budget information (if set).

```{r}
stats <- agent$get_usage_stats()
stats
```

## Generate and execute R code from natural language

`generate_execute_r_code()` lets an `Agent` translate a natural-language task description into R code, optionally validate its syntax, and (optionally) execute it.

- **code_description**: a plain-English description of the R code to generate.
- **validate**: `TRUE` to run a syntax validation step on the generated code first.
- **execute**: `TRUE` to execute the generated code (requires successful validation).
- **interactive**: if `TRUE`, shows the code and asks for confirmation before executing.
- **env**: environment where code will run when `execute = TRUE` (default `globalenv()`).

Safety notes:
- Set `validate = TRUE` and review the printed code before execution.
- Keep `interactive = TRUE` to require an explicit confirmation before running code.

```{r}
openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

r_assistant <- Agent$new(
  name = "R Code Assistant",
  instruction = "You are an expert R programmer.",
  llm_object = openai_4_1_mini
)

agent$generate_execute_r_code(
   code_description = "using ggplot2, generate a scatterplot of hwy and cty in red", 
   validate = TRUE, 
   execute = TRUE, 
   interactive = FALSE
 )
```


## Cloning an Agent

If you want to create a new agent with the exact same characteristics, you can use the `clone_agent` method. Note that the new Agent can have the same name but it'll have a different `ID`: 


```{r}
rai_agent <- Agent$new(
  name = "Rai musician",
  instruction = "You are an expert in Algerian Rai music",
  llm_object = openai_4_1_mini
)

result <- rai_agent$invoke("Give me a rai song in 1 sentence. Don't explain")

rai_agent$agent_id
rai_agent$name
rai_agent$instruction
rai_agent$messages
```


```{r}
new_rai_agent <- rai_agent$clone_agent(new_name = "Just Rai")

new_rai_agent$agent_id
new_rai_agent$name
new_rai_agent$instruction
new_rai_agent$messages
```

 
## Response Validation
         
The `validate_response()` method provides intelligent LLM-based validation of 
agent responses against custom criteria. This powerful feature uses the agent's own
LLM to evaluate whether a response meets specified validation standards, returning
both a score and detailed feedback.
         
### How it works
         
The method evaluates the response against your criteria. It returns a validation score (0-1) and determines if the response is valid based on your threshold.
         
### Parameters
         
- **prompt**: The original prompt that generated the response
- **response**: The response text to validate
- **validation_criteria**: Your validation requirements (e.g., "Must be factual and under 50 words")
- **validation_score**: Score threshold for validity (0-1, default 0.8)
         
#### Example 1: Factual Content Validation

```{r}
fact_checker <- Agent$new(
  name = "fact_checker",
  instruction = "You are a factual assistant.",
  llm_object = openai_4_1_mini
)

prompt <- "What is the capital of Algeria?"
response <- fact_checker$invoke(prompt)

validation <- fact_checker$validate_response(
  prompt = prompt,
  response = response,
  validation_criteria = "The response must be factually accurate and mention Algiers as the capital",
  validation_score = 0.8
)

validation
```

#### Example 2: Content Length and Style Validation

```{r}
content_agent <- Agent$new(
  name = "content_creator",
  instruction = "You are a creative writing assistant.",
  llm_object = openai_4_1_mini
)

prompt <- "Write a 1 sentence advertisment about an Algerian dates (the fruid)"

response <- content_agent$invoke(prompt)

validation <- content_agent$validate_response(
  prompt = prompt,
  response = response,
  validation_criteria = "Response must be under 100 words, professional tone, and highlight Algerian dates",
  validation_score = 0.75
)

validation
```

### Use Cases

- **Quality Control**: Validate responses meet content standards before 
publication
- **Factual Accuracy**: Ensure responses contain correct information
- **Style Compliance**: Check responses follow tone, length, or format 
requirements
- **Safety Filtering**: Validate content meets safety and appropriateness criteria
- **A/B Testing**: Compare response quality across different models or prompts

The validation results include the original prompt, response, criteria, score, 
feedback, and validity status, making it easy to audit and improve your agent's 
performance.

## Tools 

You can easily register one or several tools using the `register_tools` method. The tools are created using `ellmer`, consider the following example: 

```{r}
openai_4_1 <- ellmer::chat(
  name = "openai/gpt-4.1",
  credentials = function() {Sys.getenv("OPENAI_API_KEY")},
  echo = "none"
)

weather_agent <- Agent$new(
  name = "weather_assistant",
  instruction = "You are a weather assistant.",
  llm_object = openai_4_1
)

weather_function_algiers <- function() {
  msg <- glue::glue(
    "35 degrees Celcius, it's sunny and there's no precipitation."
  )
  msg
}

get_weather_in_algiers <- ellmer::tool(
  fun = weather_function_algiers,
  name = "get_weather_in_algiers",
  description = "Provide the current weather in Algiers, Algeria."
)

weather_function_berlin <- function() {
  msg <- glue::glue(
    "10 degrees Celcius, it's cold"
  )
  msg
}

get_weather_in_berlin <- ellmer::tool(
  fun = weather_function_berlin,
  name = "get_weather_in_berlin",
  description = "Provide the current weather in Berlin, Germany"
)

weather_agent$register_tools(
  tools = list(
    get_weather_in_algiers, 
    get_weather_in_berlin
  )
)
```
One can list the available tools: 

```{r}
weather_agent$list_tools()
```

After registering the tools, the Agent will use them when needed: 

```{r}
weather_agent$invoke("How's the weather in Algiers?")
```

```{r}
weather_agent$invoke("How's the weather in Berlin?")
```

One can remove one or several tool using the `remove_tools` method or remove all agents at one using the `clear_tools` method: 

```{r}
weather_agent$clear_tools()
weather_agent$list_tools()
```
### Tool Generation


The `generate_and_register_tool` method allows you to create tools from simple natural language descriptions (for example, ‚Äúcreate a tool that saves files to disk‚Äù) and automatically generates the complete R code needed to implement them. It produces a fully functional R function that encapsulates the tool‚Äôs logic, along with a complete `ellmer` tool definition that includes proper type specifications and clear parameter descriptions. 


```{r}
weather_agent$generate_and_register_tool(
  description = "create a tool that uses httr to call the open-meteo api https://open-meteo.com/en/docs to get the current weather about any city in the world"
)
```

```{r}
weather_agent$invoke(
  prompt = "what is the current weather in Tokyo?"
)
```

# LeadAgent
## Creating a multi-agents orchestraction 

We can create as many Agents as we want, the `LeadAgent` will dispatch the instructions to the agents and provide with the final answer back. Let's create three Agents, a `researcher`, a `summarizer` and a `translator`: 


```{r}

researcher <- Agent$new(
  name = "researcher",
  instruction = "You are a research assistant. Your job is to answer factual questions with detailed and accurate information. Do not answer with more than 2 lines",
  llm_object = openai_4_1_mini
)

summarizer <- Agent$new(
  name = "summarizer",
  instruction = "You are agent designed to summarise a give text into 3 distinct bullet points.",
  llm_object = openai_4_1_mini
)

translator <- Agent$new(
  name = "translator",
  instruction = "Your role is to translate a text from English to German",
  llm_object = openai_4_1_mini
)
```


Now, the most important part is to create a `LeadAgent`: 

```{r}
lead_agent <- LeadAgent$new(
  name = "Leader", 
  llm_object = openai_4_1_mini
)
```

Note that the `LeadAgent` cannot receive an `instruction` as it has already the necessary instructions. 

Next, we need to assign the Agents to `LeadAgent`, we do it as follows: 

```{r}
lead_agent$register_agents(c(researcher, summarizer, translator))

lapply(lead_agent$agents, function(x) {x$name})
```

Before executing your prompt, you can ask the `LeadAgent` to generate a plan so that you can see which `Agent` will be used for which prompt, you can do it as follows:

```{r}
prompt_to_execute <- "Tell me about the economic situation in Algeria, summarize it in 3 bullet points, then translate it into German."

plan <- lead_agent$generate_plan(prompt_to_execute)
plan
```

Now, in order now to execute the workflow, we just need to call the `invoke` method which will behind the scene delegate the prompts to suitable Agents and retrieve back the final information: 

```{r}
response <- lead_agent$invoke("Tell me about the economic situation in Algeria, summarize it in 3 bullet points, then translate it into German.")
```

```{r}
response
```

If you want to inspect the multi-agents orchestration, you have access to the `agents_interaction` object: 

```{r}
lead_agent$agents_interaction
```


The above example is extremely simple, the usefulness of `mini007` would shine in more complex processes where a multi-agent sequential orchestration has a higher value added. 


## Visualizing agent plans with `visualize_plan()`

Sometimes, before running your workflow, it is helpful to view the orchestration as a visual diagram, showing the sequence of agents and which prompt each will receive. After generating a plan, you can call `visualize_plan()`:

This function displays the agents in workflow order as labeled boxes. Hovering a box reveals the delegated prompt. The visualization uses the `DiagrammeR` package. If no plan exists, it asks you to generate one first.

```{r, eval=FALSE}
lead_agent$visualize_plan()
```


## Broadcasting 

If you want to compare several `LLM` models, the `LeadAgent` provides a `broadcast` method that allows you to send a prompt to several different agents and get the result for each agent back in order to make a comparison and potentially choose the best agent/model for the defined prompt: 

Let's go through an example: 

```{r}
openai_4_1 <- ellmer::chat(
  name = "openai/gpt-4.1",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

openai_4_1_agent <- Agent$new(
  name = "openai_4_1_agent", 
  instruction = "You are an AI assistant. Answer in 1 sentence max.", 
  llm_object = openai_4_1
)

openai_4_1_nano <- ellmer::chat(
  name = "openai/gpt-4.1-nano",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

openai_4_1_nano_agent <- Agent$new(
  name = "openai_4_1_nano_agent", 
  instruction = "You are an AI assistant. Answer in 1 sentence max.", 
  llm_object = openai_4_1_nano
)
```

```{r}

lead_agent$clear_agents() # removing previous agents
lead_agent$register_agents(c(openai_4_1_agent, openai_4_1_nano_agent))
```


```{r}
lead_agent$broadcast(prompt = "If I were Algerian, which song would I like to sing when running under the rain? how about a flower?")
```

You can also access the history of the `broadcasting` using the `broadcast_history` attribute: 


```{r}
lead_agent$broadcast_history
```

## Human In The Loop (HITL)

When executing an LLM workflow that relies on many steps, you can set `Human In The Loop` (`HITL`) trigger that will check the model's response at a specific step. You can define a `HITL` trigger after defining a `LeadAgent` as follows:

```{r}
openai_llm_object <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

lead_agent <- LeadAgent$new(
  name = "Leader", 
  llm_object = openai_llm_object
)

lead_agent$set_hitl(steps = 1)

lead_agent$hitl_steps
```
After setting the `HITL` to step 1, the workflow execution will pose and give the user 3 choices: 

1. Continue the execution of the workflow as it is;
2. Change manually the answer of the specified step and continue the execution of the workflow;
3. Stop the execution of the workflow (hard error);

Note that you can set a `HITL` at several steps, for example `lead_agent$set_hitl(steps = c(1, 2))` will set the `HITL` at step 1 and step 2. 


## Judge as a decision process 

Sometimes you want to send a prompt to several agents and pick the best answer. In order to choose the best prompt, you can also rely on the `Lead` Agent which will act a dudge and pick for you the best answer. You can use the `judge_and_choose_best_response` method as follows: 

```{r}
openai_4_1 <- ellmer::chat(
  name = "openai/gpt-4.1",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

stylist_1 <- Agent$new(
  name = "stylist",
  instruction = "You are an AI assistant. Answer in 1 sentence max.",
  llm_object = openai_4_1
)

openai_4_1_nano <- ellmer::chat(
  name = "openai/gpt-4.1-nano",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

stylist_2 <- Agent$new(
  name = "stylist2",
  instruction = "You are an AI assistant. Answer in 1 sentence max.",
  llm_object = openai_4_1_nano
)

openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  credentials = retrieve_open_ai_credential, 
  echo = "none"
)

stylist_lead_agent <- LeadAgent$new(
  name = "Stylist Leader",
  llm_object = openai_4_1_mini
)

stylist_lead_agent$register_agents(c(stylist_1, stylist_2))

best_answer <- stylist_lead_agent$judge_and_choose_best_response(
  "what's the best way to wear a blue kalvin klein shirt in winter with a pink pair of trousers?"
)

best_answer
```

## Agents Dialog

The `agents_dialog` method facilitates an intelligent two-agent collaboration process designed to refine and optimize responses through iterative dialogue. 

It enables two registered agents to take alternating turns improving each other‚Äôs outputs until a high-quality final response is reached. The method supports a configurable maximum number of iterations (default: 5) and includes a self-stopping mechanism where agents can indicate agreement by beginning their message with ‚ÄúCONSENSUS:‚Äù, followed by the final answer. 

If no consensus is achieved within the iteration limit, the lead agent automatically synthesizes a concluding response based on the conversation. Throughout the exchange, every interaction is stored within the `self$dialog_history` object. Consider the following examples: 

```{r}
ceo1 <- Agent$new(
  name = "ceo1",
  instruction = paste0(
    "You are a CEO in  a dates company based in Ouergla, Algeria, ", 
    "You want to boost their exports to Germany. "
  ),
  llm_object = openai_4_1_mini
)

ceo2 <- Agent$new(
  name = "ceo2",
  instruction = paste0(
    "You are the CEO of a dates company based in Ouergla, Algeria. ", 
    "You are considering starting a marketing compaign to boost the exports to Germany. "
  ),
  llm_object = openai_4_1_mini
)

lead_agent <- LeadAgent$new(
  name = "Leader",
  llm_object = openai_4_1_mini
)

lead_agent$register_agents(c(ceo1, ceo2))

result <- lead_agent$agents_dialog(
  prompt = "Propose a plan in 1 sentence max about a marketing strategy that will boost the export of dates to Germany for the next 2 years",
  agent_1_id = ceo1$agent_id,
  agent_2_id = ceo2$agent_id,
  max_iterations = 3
)

# Access the final response
result$final_response

# View the dialog history
result$dialog_history
```


If the instructions of the Agents differ fundamentally, they won't be able to find a consensus and the `LeadAgent` will take over and provide a final response: 


```{r}
ceo1 <- Agent$new(
  name = "ceo1",
  instruction = paste0(
    "You are a CEO in  a dates company based in Ouergla, Algeria, ", 
    "You want to boost their exports to Germany. ", 
    "You don't care about the budget. You want to spend as much as possible. "
  ),
  llm_object = openai_4_1_mini
)

ceo2 <- Agent$new(
  name = "ceo2",
  instruction = paste0(
    "You are the CEO of a dates company based in Ouergla, Algeria. ", 
    "You are considering starting a marketing compaign to boost the exports to Germany. ",
    "For you the marketing budget is super important and you don't want to spend too much. "
  ),
  llm_object = openai_4_1_mini
)

lead_agent <- LeadAgent$new(
  name = "Leader",
  llm_object = openai_4_1_mini
)

lead_agent$register_agents(c(ceo1, ceo2))

result <- lead_agent$agents_dialog(
  prompt = "Propose a plan in 1 sentence max about a marketing strategy that will boost the export of dates to Germany for the next 2 years",
  agent_1_id = ceo1$agent_id,
  agent_2_id = ceo2$agent_id,
  max_iterations = 3
)

# Access the final response
result$final_response

# View the dialog history
result$dialog_history
```


## Code of Conduct

Please note that the mini007 project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/1/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.
