---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# mini007 <a><img src='man/figures/mini007cute.png' align="right" height="200" /></a>

<!-- badges: start -->
<!-- badges: end -->

`mini007` provides a lightweight and extensible framework for multi-agents orchestration processes capable of decomposing complex tasks and assigning them to specialized agents. 

Each `agent` is an extension of an `ellmer` object. `mini007` relies heavily on the excellent `ellmer` package but aims to make it easy to create a process where multiple specialized agents help each other sequentially in order to execute a task.

`mini007` provides two types of agents: 

- A normal `Agent` containing a name and an instruction, 
- and a `LeadAgent` which will take a complex prompt, split it, assign to the adequate agents and retrieve the response. 

#### Highlights

🧠 Memory and identity for each agent via `uuid` and message history.

⚙️ Built-in task decomposition and delegation via `LLM`.

🔄 Agent-to-agent orchestration with result chaining.

🌐 Compatible with any chat model supported by `ellmer`.

🧑 Possibility to set a Human In The Loop (`HITL`) at various execution steps

You can install `mini007` from `CRAN` with: 

```{r, eval=FALSE}
install.packages("mini007")
```

```{r}
library(mini007)
```


### Creating an Agent

An Agent is built upon an LLM object created by the `ellmer` package, in the following examples, we'll work with the `OpenAI` models, however you can use any model/combination of models you want: 

```{r}
# no need to provide the system prompt, it will be set when creating the
# agent (see the 'instruction' parameter)

openai_4_1_mini <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  api_key = Sys.getenv("OPENAI_API_KEY"), 
  echo = "none"
)

```

After initializing the `ellmer` LLM object, creating the Agent is straightforward: 

```{r}
polar_bear_researcher <- Agent$new(
  name = "POLAR BEAR RESEARCHER",
  instruction = "You are an expert in polar bears, you task is to collect information about polar bears. Answer in 1 sentence max.",
  llm_object = openai_4_1_mini
)
```

Each created Agent has an `agent_id` (among other meta information): 

```{r}
polar_bear_researcher$agent_id
```

At any time, you can tweak the `llm_object`: 
```{r}
polar_bear_researcher$llm_object
```

An agent can provide the answer to a prompt using the `invoke` method: 

```{r}
polar_bear_researcher$invoke("Are polar bears dangerous for humans?")
```


You can also retrieve a list that displays the history of the agent:

```{r}
polar_bear_researcher$messages
```
Or the `ellmer` way: 

```{r}
polar_bear_researcher$llm_object
```


### Creating a multi-agents orchestraction 

We can create as many Agents as we want, the `LeadAgent` will dispatch the instructions to the agents and provide with the final answer back. Let's create three Agents, a `researcher`, a `summarizer` and a `translator`: 


```{r}

researcher <- Agent$new(
  name = "researcher",
  instruction = "You are a research assistant. Your job is to answer factual questions with detailed and accurate information. Do not answer with more than 2 lines",
  llm_object = openai_4_1_mini
)

summarizer <- Agent$new(
  name = "summarizer",
  instruction = "You are agent designed to summarise a give text into 3 distinct bullet points.",
  llm_object = openai_4_1_mini
)

translator <- Agent$new(
  name = "translator",
  instruction = "Your role is to translate a text from English to German",
  llm_object = openai_4_1_mini
)
```


Now, the most important part is to create a `LeadAgent`: 

```{r}
lead_agent <- LeadAgent$new(
  name = "Leader", 
  llm_object = openai_4_1_mini
)
```

Note that the `LeadAgent` cannot receive an `instruction` as it has already the necessary instructions. 

Next, we need to assign the Agents to `LeadAgent`, we do it as follows: 

```{r}
lead_agent$register_agents(c(researcher, summarizer, translator))
lead_agent$agents
```

Before executing your prompt, you can ask the `LeadAgent` to generate a plan so that you can see which `Agent` will be used for which prompt, you can do it as follows:

```{r}
prompt_to_execute <- "Tell me about the economic situation in Algeria, summarize it in 3 bullet points, then translate it into German."

plan <- lead_agent$generate_plan(prompt_to_execute)
plan
```

Now, in order now to execute the workflow, we just need to call the `invoke` method which will behind the scene delegate the prompts to suitable Agents and retrieve back the final information: 

```{r}
response <- lead_agent$invoke("Tell me about the economic situation in Algeria, summarize it in 3 bullet points, then translate it into German.")
```

```{r}
response
```

If you want to inspect the multi-agents orchestration, you have access to the `agents_interaction` object: 

```{r}
lead_agent$agents_interaction
```


The above example is extremely simple, the usefulness of `mini007` would shine in more complex processes where a multi-agent sequential orchestration has a higher value added. 


## Broadcasting 

If you want to compare several `LLM` models, the `LeadAgent` provides a `broadcast` method that allows you to send a prompt to several different agents and get the result for each agent back in order to make a comparison and potentially choose the best agent/model for the defined prompt: 

Let's go through an example: 

```{r}
openai_4_1 <- ellmer::chat(
  name = "openai/gpt-4.1",
  api_key = Sys.getenv("OPENAI_API_KEY"), 
  echo = "none"
)

openai_4_1_agent <- Agent$new(
  name = "openai_4_1_agent", 
  instruction = "You are an AI assistant. Answer in 1 sentence max.", 
  llm_object = openai_4_1
)

openai_4_1_nano <- ellmer::chat(
  name = "openai/gpt-4.1-nano",
  api_key = Sys.getenv("OPENAI_API_KEY"), 
  echo = "none"
)

openai_4_1_nano_agent <- Agent$new(
  name = "openai_4_1_nano_agent", 
  instruction = "You are an AI assistant. Answer in 1 sentence max.", 
  llm_object = openai_4_1_nano
)
```

```{r}

lead_agent$clear_agents() # removing previous agents
lead_agent$register_agents(c(openai_4_1_agent, openai_4_1_nano_agent))
```


```{r}
lead_agent$broadcast(prompt = "If I were Algerian, which song would I like to sing when running under the rain? how about a flower?")
```

You can also access the history of the `broadcasting` using the `broadcast_history` attribute: 


```{r}
lead_agent$broadcast_history
```


## Tool specification 

As mentioned previously, an `Agent` is an extension of an `ellmer` object. As such, you can define a tool that will be used, the exact same way as in `ellmer`. Suppose, we want to get the weather in `Algiers` through a function (Tool). Let's first create the `Agents`: 


```{r}
openai_llm_object <- ellmer::chat(
  name = "openai/gpt-4.1-mini",
  api_key = Sys.getenv("OPENAI_API_KEY"), 
  echo = "none"
)

assistant <- Agent$new(
  name = "assistant",
  instruction = "You are an AI assistant that answers question. Do not answer with more than 1 sentence.",
  llm_object = openai_llm_object
)

weather_assistant <- Agent$new(
  name = "weather_assistant",
  instruction = "You role is to provide weather assistance.",
  llm_object = openai_llm_object
)
```

Now, let's define the `tool` that we'll be using, using `ellmer` it's quite straightforward: 

```{r}
get_weather_in_algiers <- ellmer::tool(
  function() {
    "35 degrees Celcius, it's sunny and there's no precipitation."
  },
  name = "get_weather_in_algiers",
  description = "Provide the current weather in Algiers, Algeria."
)
```

Our `tool` defined, the next step is to register it within the suitable `Agent`, in our case, the `weather_assistant` `Agent`: 

```{r}
weather_assistant$llm_object$register_tool(get_weather_in_algiers)
```

That's it, now the last step is to create the `LeadAgent`, register the `Agents` that we need and call the `invoke` method:  

```{r}
lead_agent <- LeadAgent$new(
  name = "Leader", 
  llm_object = openai_llm_object
)

lead_agent$register_agents(c(assistant, weather_assistant))

lead_agent$invoke(
  "Tell me about the economic situation in Algeria, then tell me how's the weather in Algiers?"
)
```


## Human In The Loop (HITL)

When executing an LLM workflow that relies on many steps, you can set `Human In The Loop` (`HITL`) trigger that will check the model's response at a specific step. You can define a `HITL` trigger after defining a `LeadAgent` as follows:

```{r}
lead_agent <- LeadAgent$new(
  name = "Leader", 
  llm_object = openai_llm_object
)

lead_agent$set_hitl(steps = 1)

lead_agent$hitl_steps
```
After setting the `HITL` to step 1, the workflow execution will pose and give the user 3 choices: 

1. Continue the execution of the workflow as it is;
2. Change manually the answer of the specified step and continue the execution of the workflow;
3. Stop the execution of the workflow (hard error);

Note that you can set a `HITL` at several steps, for example `lead_agent$set_hitl(steps = c(1, 2))` will set the `HITL` at step 1 and step 2. 

## Code of Conduct

Please note that the mini007 project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/1/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.
